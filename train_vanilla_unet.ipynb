{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this to True if you want to train the model.\n",
    "Train = False\n",
    "#Train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Train == True :\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, target_paths,train=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.files = os.listdir(self.image_paths)\n",
    "        self.lables = os.listdir(self.target_paths)\n",
    "    \n",
    "    def transform(self, image, mask):\n",
    "            \n",
    "        # Random crop\n",
    "        if random.random() > 0.5:\n",
    "            affine_params=transforms.RandomAffine(180).get_params((-90, 90), (0.1,0.1), (0.8, 1.2), None, image.size)\n",
    "            image = TF.affine(image, *affine_params)\n",
    "            mask = TF.affine(mask, *affine_params)    \n",
    "\n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        \n",
    "        # Normalize the image\n",
    "        norm = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        image = norm(image)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        label_name = self.lables[idx]\n",
    "        \n",
    "        image = Image.open(os.path.join(self.image_paths,img_name))\n",
    "        mask = Image.open(os.path.join(self.target_paths,label_name))        \n",
    "       \n",
    "        x, y= self.transform(image,mask)\n",
    "        return x, y\n",
    "     \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'Trainset3/Image'\n",
    "label_paht = 'Trainset3/Label'\n",
    "dataset = MyDataset(image_path,label_paht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset) #just check the number of images on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can change the batch_size here\n",
    "#for the batch_size, the smaller tends to be better.\n",
    "#dataloader = DataLoader(dataset=dataset, batch_size=8, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length=int(0.90* len(dataset))\n",
    "test_length=len(dataset)-train_length\n",
    "train_dataset,val_dataset = torch.utils.data.random_split(dataset, (train_length,test_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=True, pin_memory=True)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=8, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model structure thanks to https://github.com/mateuszbuda/brain-segmentation-pytorch\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=32):\n",
    "        super(UNet, self).__init__()\n",
    "        def conv_block(in_channels, features):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, features, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(features),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(features),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #nn.Dropout(p=0.2)\n",
    "                #nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        self.enc1 = conv_block(in_channels,features)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.enc2 = conv_block(features,features*2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.enc3 = conv_block(features*2,features*4)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.enc4 = conv_block(features*4,features*8)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.bottleneck = conv_block(features*8,features*16)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.dec4 = conv_block((features * 8) * 2, features * 8)\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block((features * 4) * 2, features * 4)\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block((features * 2) * 2, features*2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(features * 2, features)\n",
    "        self.out = nn.Conv2d(features, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.maxpool1(e1))\n",
    "        e3 = self.enc3(self.maxpool2(e2))\n",
    "        e4 = self.enc4(self.maxpool3(e3))\n",
    "        #e4 = torch.nn.functional.dropout(e4,p=0.3)\n",
    "        bottleneck = self.bottleneck(self.maxpool4(e4))\n",
    "        \n",
    "        d4 = self.upconv4(bottleneck)\n",
    "        d4 = torch.cat((d4,e4), dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        #d4 = torch.nn.functional.dropout(d4,p=0.2)\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3,e3), dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        #d3 = torch.nn.functional.dropout(d3,p=0.2)\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2,e2), dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat((d1,e1), dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        output = self.out(d1)\n",
    "        #output = torch.sigmoid(output) #sigmoid as a last activation layer.\n",
    "        #no need sigmoid if use BCEwithlogitloss\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet()\n",
    "unet = unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(pred, target,smooth=1):\n",
    "    \n",
    "  \n",
    "    pred = torch.sigmoid(pred)\n",
    "    if target.sum().item() == 0:\n",
    "        pred = (pred>0.5).float()\n",
    "    N = target.size(0)\n",
    "    pred_flat = pred.view(N, -1)\n",
    "    gt_flat = target.view(N, -1)\n",
    " \n",
    "    intersection = (pred_flat * gt_flat).sum(1)\n",
    "    #unionset = pred_flat.sum(1) + gt_flat.sum(1)\n",
    "    unionset = (pred_flat ** 2).sum(1) + (gt_flat ** 2).sum(1)\n",
    "    loss = ((2. * (intersection)) + smooth) / (unionset + smooth)\n",
    "    return loss.sum() / N\n",
    "    \n",
    "''' initialize an empty list when Scores is called, append the list with dice scores\n",
    "for every batch, at the end of epoch calculates mean of the dice scores'''\n",
    "class Scores:\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_dice_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = outputs\n",
    "        dice= dice_score(probs, targets)\n",
    "        self.base_dice_scores.append(dice.item())\n",
    "\n",
    "    def get_metrics(self): \n",
    "        dice = sum(self.base_dice_scores)/len(self.base_dice_scores)\n",
    "        return dice\n",
    "\n",
    "'''return dice score for epoch when called'''\n",
    "def epoch_log(phase, epoch_loss, measure):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices= measure.get_metrics()                          \n",
    "    print(\"%s: Loss: %0.4f |dice: %0.4f\" % (phase, epoch_loss, dices))\n",
    "    return dices\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    __name__ = 'dice_loss'\n",
    " \n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    " \n",
    "    def forward(self, pred, target):\n",
    "        return 1 - dice_score(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n",
      "Train: Loss: 0.3749 |dice: 0.2419\n",
      "Validation: Loss: 0.1256 |dice: 0.2796\n",
      "epoch1\n",
      "Train: Loss: 0.2799 |dice: 0.3553\n",
      "Validation: Loss: 0.1355 |dice: 0.3612\n",
      "epoch2\n",
      "Train: Loss: 0.2429 |dice: 0.3992\n",
      "Validation: Loss: 0.1054 |dice: 0.3979\n",
      "epoch3\n",
      "Train: Loss: 0.2295 |dice: 0.4185\n",
      "Validation: Loss: 0.1166 |dice: 0.3992\n",
      "epoch4\n",
      "Train: Loss: 0.2197 |dice: 0.4304\n",
      "Validation: Loss: 0.1392 |dice: 0.3699\n",
      "epoch5\n",
      "Train: Loss: 0.2147 |dice: 0.4359\n",
      "Validation: Loss: 0.0908 |dice: 0.4239\n",
      "epoch6\n",
      "Train: Loss: 0.2101 |dice: 0.4404\n",
      "Validation: Loss: 0.1368 |dice: 0.3891\n",
      "epoch7\n",
      "Train: Loss: 0.2068 |dice: 0.4464\n",
      "Validation: Loss: 0.1255 |dice: 0.4037\n",
      "epoch8\n",
      "Train: Loss: 0.2042 |dice: 0.4495\n",
      "Validation: Loss: 0.1020 |dice: 0.4264\n",
      "epoch9\n",
      "Train: Loss: 0.2014 |dice: 0.4538\n",
      "Validation: Loss: 0.0952 |dice: 0.4370\n",
      "epoch10\n",
      "Train: Loss: 0.1903 |dice: 0.4669\n",
      "Validation: Loss: 0.1007 |dice: 0.4385\n",
      "epoch11\n",
      "Train: Loss: 0.1880 |dice: 0.4698\n",
      "Validation: Loss: 0.0914 |dice: 0.4525\n",
      "epoch12\n",
      "Train: Loss: 0.1871 |dice: 0.4701\n",
      "Validation: Loss: 0.0914 |dice: 0.4521\n",
      "epoch13\n",
      "Train: Loss: 0.1858 |dice: 0.4724\n",
      "Validation: Loss: 0.0906 |dice: 0.4509\n",
      "epoch14\n",
      "Train: Loss: 0.1848 |dice: 0.4746\n",
      "Validation: Loss: 0.0921 |dice: 0.4488\n",
      "epoch15\n",
      "Train: Loss: 0.1839 |dice: 0.4742\n",
      "Validation: Loss: 0.0910 |dice: 0.4522\n",
      "epoch16\n",
      "Train: Loss: 0.1831 |dice: 0.4758\n",
      "Validation: Loss: 0.0892 |dice: 0.4549\n",
      "epoch17\n",
      "Train: Loss: 0.1827 |dice: 0.4751\n",
      "Validation: Loss: 0.0959 |dice: 0.4479\n",
      "epoch18\n",
      "Train: Loss: 0.1823 |dice: 0.4766\n",
      "Validation: Loss: 0.0937 |dice: 0.4455\n",
      "epoch19\n",
      "Train: Loss: 0.1818 |dice: 0.4765\n",
      "Validation: Loss: 0.0930 |dice: 0.4490\n",
      "epoch20\n",
      "Train: Loss: 0.1808 |dice: 0.4769\n",
      "Validation: Loss: 0.0941 |dice: 0.4510\n",
      "epoch21\n",
      "Train: Loss: 0.1799 |dice: 0.4775\n",
      "Validation: Loss: 0.0894 |dice: 0.4504\n",
      "epoch22\n",
      "Train: Loss: 0.1810 |dice: 0.4796\n",
      "Validation: Loss: 0.0966 |dice: 0.4464\n",
      "epoch23\n",
      "Train: Loss: 0.1790 |dice: 0.4792\n",
      "Validation: Loss: 0.0923 |dice: 0.4512\n",
      "epoch24\n",
      "Train: Loss: 0.1786 |dice: 0.4807\n",
      "Validation: Loss: 0.0852 |dice: 0.4638\n",
      "epoch25\n",
      "Train: Loss: 0.1783 |dice: 0.4806\n",
      "Validation: Loss: 0.0849 |dice: 0.4645\n",
      "epoch26\n",
      "Train: Loss: 0.1774 |dice: 0.4810\n",
      "Validation: Loss: 0.0897 |dice: 0.4546\n",
      "epoch27\n",
      "Train: Loss: 0.1771 |dice: 0.4812\n",
      "Validation: Loss: 0.0931 |dice: 0.4536\n",
      "epoch28\n",
      "Train: Loss: 0.1751 |dice: 0.4821\n",
      "Validation: Loss: 0.0796 |dice: 0.4656\n",
      "epoch29\n",
      "Train: Loss: 0.1766 |dice: 0.4842\n",
      "Validation: Loss: 0.0862 |dice: 0.4626\n",
      "epoch30\n",
      "Train: Loss: 0.1758 |dice: 0.4845\n",
      "Validation: Loss: 0.0933 |dice: 0.4524\n",
      "epoch31\n",
      "Train: Loss: 0.1758 |dice: 0.4846\n",
      "Validation: Loss: 0.0884 |dice: 0.4571\n",
      "epoch32\n",
      "Train: Loss: 0.1756 |dice: 0.4845\n",
      "Validation: Loss: 0.0882 |dice: 0.4579\n",
      "epoch33\n",
      "Train: Loss: 0.1744 |dice: 0.4857\n",
      "Validation: Loss: 0.0871 |dice: 0.4555\n",
      "epoch34\n",
      "Train: Loss: 0.1739 |dice: 0.4867\n",
      "Validation: Loss: 0.0849 |dice: 0.4624\n",
      "epoch35\n",
      "Train: Loss: 0.1739 |dice: 0.4877\n",
      "Validation: Loss: 0.0889 |dice: 0.4585\n",
      "epoch36\n",
      "Train: Loss: 0.1731 |dice: 0.4873\n",
      "Validation: Loss: 0.0946 |dice: 0.4536\n",
      "epoch37\n",
      "Train: Loss: 0.1722 |dice: 0.4885\n",
      "Validation: Loss: 0.0891 |dice: 0.4573\n",
      "epoch38\n",
      "Train: Loss: 0.1712 |dice: 0.4887\n",
      "Validation: Loss: 0.0904 |dice: 0.4542\n",
      "epoch39\n",
      "Train: Loss: 0.1714 |dice: 0.4902\n",
      "Validation: Loss: 0.0890 |dice: 0.4600\n",
      "epoch40\n",
      "Train: Loss: 0.1715 |dice: 0.4879\n",
      "Validation: Loss: 0.0787 |dice: 0.4677\n",
      "epoch41\n",
      "Train: Loss: 0.1708 |dice: 0.4914\n",
      "Validation: Loss: 0.0873 |dice: 0.4616\n",
      "epoch42\n",
      "Train: Loss: 0.1710 |dice: 0.4918\n",
      "Validation: Loss: 0.0902 |dice: 0.4551\n",
      "epoch43\n",
      "Train: Loss: 0.1708 |dice: 0.4913\n",
      "Validation: Loss: 0.0860 |dice: 0.4604\n",
      "epoch44\n",
      "Train: Loss: 0.1703 |dice: 0.4912\n",
      "Validation: Loss: 0.0947 |dice: 0.4510\n",
      "epoch45\n",
      "Train: Loss: 0.1692 |dice: 0.4914\n",
      "Validation: Loss: 0.0908 |dice: 0.4529\n",
      "epoch46\n",
      "Train: Loss: 0.1695 |dice: 0.4928\n",
      "Validation: Loss: 0.0848 |dice: 0.4578\n",
      "epoch47\n",
      "Train: Loss: 0.1680 |dice: 0.4938\n",
      "Validation: Loss: 0.0804 |dice: 0.4648\n",
      "epoch48\n",
      "Train: Loss: 0.1679 |dice: 0.4944\n",
      "Validation: Loss: 0.0826 |dice: 0.4676\n",
      "epoch49\n",
      "Train: Loss: 0.1681 |dice: 0.4955\n",
      "Validation: Loss: 0.0826 |dice: 0.4669\n",
      "epoch50\n",
      "Train: Loss: 0.1670 |dice: 0.4957\n",
      "Validation: Loss: 0.0946 |dice: 0.4468\n",
      "epoch51\n",
      "Train: Loss: 0.1669 |dice: 0.4940\n",
      "Validation: Loss: 0.0972 |dice: 0.4481\n",
      "epoch52\n",
      "Train: Loss: 0.1659 |dice: 0.4971\n",
      "Validation: Loss: 0.0865 |dice: 0.4675\n",
      "epoch53\n",
      "Train: Loss: 0.1656 |dice: 0.4966\n",
      "Validation: Loss: 0.0820 |dice: 0.4701\n",
      "epoch54\n",
      "Train: Loss: 0.1655 |dice: 0.4975\n",
      "Validation: Loss: 0.0847 |dice: 0.4648\n",
      "epoch55\n",
      "Train: Loss: 0.1650 |dice: 0.4985\n",
      "Validation: Loss: 0.0866 |dice: 0.4632\n",
      "epoch56\n",
      "Train: Loss: 0.1643 |dice: 0.5003\n",
      "Validation: Loss: 0.0906 |dice: 0.4556\n",
      "epoch57\n",
      "Train: Loss: 0.1646 |dice: 0.4992\n",
      "Validation: Loss: 0.0877 |dice: 0.4576\n",
      "epoch58\n",
      "Train: Loss: 0.1634 |dice: 0.5003\n",
      "Validation: Loss: 0.0893 |dice: 0.4594\n",
      "epoch59\n",
      "Train: Loss: 0.1632 |dice: 0.5017\n",
      "Validation: Loss: 0.0862 |dice: 0.4635\n",
      "epoch60\n",
      "Train: Loss: 0.1622 |dice: 0.5029\n",
      "Validation: Loss: 0.0867 |dice: 0.4608\n",
      "epoch61\n",
      "Train: Loss: 0.1623 |dice: 0.5033\n",
      "Validation: Loss: 0.0894 |dice: 0.4525\n",
      "epoch62\n",
      "Train: Loss: 0.1611 |dice: 0.5027\n",
      "Validation: Loss: 0.0923 |dice: 0.4551\n",
      "epoch63\n",
      "Train: Loss: 0.1612 |dice: 0.5037\n",
      "Validation: Loss: 0.0860 |dice: 0.4605\n",
      "epoch64\n",
      "Train: Loss: 0.1597 |dice: 0.5040\n",
      "Validation: Loss: 0.0926 |dice: 0.4552\n",
      "epoch65\n",
      "Train: Loss: 0.1591 |dice: 0.5054\n",
      "Validation: Loss: 0.0830 |dice: 0.4674\n",
      "epoch66\n",
      "Train: Loss: 0.1593 |dice: 0.5080\n",
      "Validation: Loss: 0.0817 |dice: 0.4674\n",
      "epoch67\n",
      "Train: Loss: 0.1595 |dice: 0.5056\n",
      "Validation: Loss: 0.0910 |dice: 0.4537\n",
      "epoch68\n",
      "Train: Loss: 0.1586 |dice: 0.5075\n",
      "Validation: Loss: 0.0875 |dice: 0.4590\n",
      "epoch69\n",
      "Train: Loss: 0.1581 |dice: 0.5062\n",
      "Validation: Loss: 0.0816 |dice: 0.4711\n",
      "epoch70\n",
      "Train: Loss: 0.1552 |dice: 0.5114\n",
      "Validation: Loss: 0.0868 |dice: 0.4619\n",
      "epoch71\n",
      "Train: Loss: 0.1556 |dice: 0.5131\n",
      "Validation: Loss: 0.0860 |dice: 0.4639\n",
      "epoch72\n",
      "Train: Loss: 0.1543 |dice: 0.5139\n",
      "Validation: Loss: 0.0841 |dice: 0.4713\n",
      "epoch73\n",
      "Train: Loss: 0.1551 |dice: 0.5145\n",
      "Validation: Loss: 0.0843 |dice: 0.4672\n",
      "epoch74\n",
      "Train: Loss: 0.1546 |dice: 0.5147\n",
      "Validation: Loss: 0.0822 |dice: 0.4684\n",
      "epoch75\n",
      "Train: Loss: 0.1556 |dice: 0.5127\n",
      "Validation: Loss: 0.0849 |dice: 0.4652\n",
      "epoch76\n",
      "Train: Loss: 0.1543 |dice: 0.5167\n",
      "Validation: Loss: 0.0864 |dice: 0.4597\n",
      "epoch77\n",
      "Train: Loss: 0.1546 |dice: 0.5146\n",
      "Validation: Loss: 0.0855 |dice: 0.4664\n",
      "epoch78\n",
      "Train: Loss: 0.1535 |dice: 0.5156\n",
      "Validation: Loss: 0.0839 |dice: 0.4665\n",
      "epoch79\n",
      "Train: Loss: 0.1540 |dice: 0.5157\n",
      "Validation: Loss: 0.0826 |dice: 0.4736\n",
      "epoch80\n",
      "Train: Loss: 0.1527 |dice: 0.5174\n",
      "Validation: Loss: 0.0820 |dice: 0.4711\n",
      "epoch81\n",
      "Train: Loss: 0.1539 |dice: 0.5165\n",
      "Validation: Loss: 0.0825 |dice: 0.4682\n",
      "epoch82\n",
      "Train: Loss: 0.1547 |dice: 0.5140\n",
      "Validation: Loss: 0.0841 |dice: 0.4686\n",
      "epoch83\n",
      "Train: Loss: 0.1540 |dice: 0.5159\n",
      "Validation: Loss: 0.0844 |dice: 0.4674\n",
      "epoch84\n",
      "Train: Loss: 0.1540 |dice: 0.5144\n",
      "Validation: Loss: 0.0849 |dice: 0.4643\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "if Train == True:\n",
    "    \n",
    "    optimizer = torch.optim.Adam(unet.parameters(), lr=0.01)\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75,125], gamma=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,70], gamma=0.1)\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(19.))\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(5.))\n",
    "    criterion_val = torch.nn.BCEWithLogitsLoss()\n",
    "    #criterion = DiceLoss().to(device)\n",
    "    n_epochs = 85\n",
    "    epoch_train_loss = []\n",
    "    epoch_train_dice = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_dice = []\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_score=Scores('Train',epoch)\n",
    "        val_score=Scores('Validation',epoch)\n",
    "        loss_train_avg = 0\n",
    "        loss_val_avg = 0\n",
    "        unet.train()\n",
    "        print('epoch' + str(epoch))\n",
    "        for i, data in enumerate(trainloader,0):\n",
    "            optimizer.zero_grad()\n",
    "            img,mask = data\n",
    "            img = img.to(device, dtype=torch.float)\n",
    "            mask = mask.to(device, dtype=torch.float)\n",
    "            output = unet(img)\n",
    "            train_score.update(mask,output)\n",
    "            train_loss = criterion(output,mask)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train_avg = loss_train_avg+train_loss.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            unet.eval()\n",
    "            for i, data in enumerate(valloader,0):\n",
    "                optimizer.zero_grad()\n",
    "                img,mask = data\n",
    "                img = img.to(device, dtype=torch.float)\n",
    "                mask = mask.to(device, dtype=torch.float)\n",
    "                output = unet(img)\n",
    "                val_score.update(mask,output)\n",
    "                val_loss = criterion_val(output,mask)\n",
    "                loss_val_avg = loss_val_avg+val_loss.item()\n",
    "            \n",
    "        \n",
    "            \n",
    "        loss_ep_train = loss_train_avg/len(trainloader)\n",
    "        dice_train = epoch_log('Train',loss_ep_train, train_score)\n",
    "        epoch_train_loss.append(loss_ep_train)\n",
    "        epoch_train_dice.append(dice_train)\n",
    "        \n",
    "        loss_ep_val = loss_val_avg/len(valloader)\n",
    "        dice_val = epoch_log('Validation',loss_ep_val, val_score)\n",
    "        epoch_val_loss.append(loss_ep_val)\n",
    "        epoch_val_dice.append(dice_val)\n",
    "        \n",
    "        scheduler.step()\n",
    "        '''\n",
    "        if loss_ep_val < best_val_loss:\n",
    "            print(\"******** New optimal found, saving state ********\")\n",
    "            best_val_loss = loss_ep_val\n",
    "            state = {\n",
    "                \"state_dict\": unet.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, 'unet_tf_1.pth')\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Train == True:\n",
    "    np.save('train_epoch_loss_tf_1', epoch_train_loss)\n",
    "    np.save('train_epoch_dice_tf_1', epoch_train_dice)\n",
    "    np.save('val_epoch_loss_tf_1', epoch_val_loss)\n",
    "    np.save('val_epoch_dice_tf_1', epoch_val_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Train == True:\n",
    "    state = {\n",
    "                \"state_dict\": unet.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "    torch.save(state, 'unet_tf_1.pth')\n",
    "else:\n",
    "    unet = UNet()\n",
    "    model = torch.load('unet_tf_1.pth', map_location=torch.device('cpu'))\n",
    "    unet.load_state_dict(model['state_dict'])\n",
    "    unet.to(device)\n",
    "    unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_test(pred, target,smooth=1e-5):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred*target).sum()\n",
    "    union = (pred ** 2).sum() + (target ** 2).sum()\n",
    "    return (((2. * intersection)+smooth)/(union+smooth)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_test2(pred, target,smooth=1):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    N = target.size(0)\n",
    "    pred_flat = pred.view(N, -1)\n",
    "    gt_flat = target.view(N, -1)\n",
    "    if target.sum().item() == 0:\n",
    "        pred = (pred>0.5).float()\n",
    "    pred = (pred>0.3).float()\n",
    "    intersection = (pred_flat * gt_flat).sum(1)\n",
    "    unionset = (pred_flat ** 2).sum(1) + (gt_flat ** 2).sum(1)\n",
    "    loss = ((2. * (intersection)) + smooth) / (unionset + smooth)\n",
    "    return loss.sum() / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = 'Trainset2/Test/60.PNG' #path to the image that we want to test\n",
    "TEST_Label = 'Trainset2/Testlabel/60.PNG'\n",
    "img_input = Image.open(TEST_PATH)\n",
    "img_input = np.array(img_input)\n",
    "img_input = TF.to_tensor(img_input).unsqueeze(dim=0)\n",
    "img_input = img_input.to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = unet(img_input)\n",
    "target = Image.open(TEST_Label)\n",
    "target = np.array(target)\n",
    "target = TF.to_tensor(target).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7734811305999756"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_test(output.cpu(),target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7735, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_test2(output.cpu(),target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.sigmoid(output)\n",
    "output = (output>0.3).float()\n",
    "predicted = output[0].squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21149cd20c8>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(predicted.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_array = predicted.detach().cpu().numpy()\n",
    "G = np.zeros((256,256,3))\n",
    "G[predicted_array>0.3] = [1,1,1]\n",
    "G[predicted_array<0.3] = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_img):\n",
    "    device = torch.device('cpu')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "    img_input = np.array(test_img)\n",
    "    img_input = TF.to_tensor(img_input).unsqueeze(dim=0)\n",
    "    img_input = img_input.to(device, dtype=torch.float)\n",
    "    output = unet(img_input)\n",
    "    output = torch.sigmoid(output)\n",
    "    predicted = output[0].squeeze(dim=0)\n",
    "    predicted_array = predicted.detach().cpu().numpy()\n",
    "    \n",
    "    #G = np.zeros((256,256,3))\n",
    "    #G[predicted_array>-6] = [1,1,1]\n",
    "    #G[predicted_array<-6] = [0,0,0]\n",
    "    return predicted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_predict(model,test_img_path):\n",
    "    test_img = Image.open(test_img_path)\n",
    "    (width,height)=test_img.size\n",
    "    size = 256 #size of extracted images\n",
    "    col = width//size #number of col\n",
    "    row = height//size #number of row\n",
    "    #output_img = np.zeros((size*row,size*col,3))\n",
    "    output_img = np.zeros((size*row,size*col))\n",
    "    for r in range(row):\n",
    "            for c in range(col):\n",
    "                box = (size*c,size*r,size*c+size,size*r+size) #upper left and lower right of the image\n",
    "                img_crop = test_img.crop(box)\n",
    "                predict_img = predict(model,img_crop)\n",
    "                #output_img[size*r:size*r+size,size*c:size*c+size,:]=predict_img\n",
    "                output_img[size*r:size*r+size,size*c:size*c+size]=predict_img\n",
    "                \n",
    "    return output_img\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = 'Trainset2/Test_/BS_1.PNG'\n",
    "out1 = crop_predict(unet,TEST_PATH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out1,cmap='gray',interpolation='nearest')\n",
    "plt.imsave('BS_1_result_1_500iter.png',out1,cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
